{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "max_length = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from pprint import pprint\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# See https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264\n",
    "\n",
    "# Uncomment this to stop corner printing and see full/verbatim\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "\n",
    "def generate_nested_sequence(length, min_seglen=5, max_seglen=10):\n",
    "    \"\"\"Generate low-high-low sequence, with indexes of the first/last high/middle elements\"\"\"\n",
    "\n",
    "    # Low (1-5) vs. High (6-10)\n",
    "    seq_before = [(random.randint(1,5)) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq_during = [(random.randint(6,10)) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq_after = [random.randint(1,5) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq = seq_before + seq_during + seq_after\n",
    "\n",
    "    # Pad it up to max len with 0's\n",
    "    seq = seq + ([0] * (length - len(seq)))\n",
    "    return [seq, len(seq_before), len(seq_before) + len(seq_during)-1]\n",
    "\n",
    "\n",
    "def create_one_hot(length, index):\n",
    "    \"\"\"Returns 1 at the index positions; can be scaled by client\"\"\"\n",
    "    a = np.zeros([length])\n",
    "    a[index] = 1.0\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_lstm_state(cell):\n",
    "    \"\"\"Centralize definition of 'state', to swap .c and .h if desired\"\"\"\n",
    "    return cell.c\n",
    "\n",
    "\n",
    "def print_pointer(arr, first, second):\n",
    "    \"\"\"Pretty print the array, along with pointers to the first/second indices\"\"\"\n",
    "    first_string = \" \".join([(\" \" * (2 - len(str(x))) + str(x)) for x in arr])\n",
    "    print(first_string)\n",
    "    second_array = [\"  \"] * len(arr)\n",
    "    second_array[first] = \"^1\"\n",
    "    second_array[second] = \"^2\"\n",
    "    if (first == second):\n",
    "        second_array[first] = \"^B\"\n",
    "    second_string = \" \" + \" \".join([x for x in second_array])\n",
    "    print(second_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_of_indices, blend_dim, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size               # B\n",
    "        self.input_dim = input_dim                 # I\n",
    "        self.hidden_size = hidden_size             # H\n",
    "        self.num_of_indices = num_of_indices       # N\n",
    "        self.blend_dim = blend_dim                 # D\n",
    "                \n",
    "        self.encode = nn.LSTMCell(input_dim, hidden_size)\n",
    "        self.decode = nn.LSTMCell(input_dim, hidden_size)\n",
    "        self.blend_decoder = nn.Linear(hidden_size, blend_dim)\n",
    "        self.blend_encoder = nn.Linear(hidden_size, blend_dim)\n",
    "        self.scale_blend = nn.Linear(blend_dim, input_dim)\n",
    "        \n",
    "    def zero_hidden_state(self):\n",
    "        return Variable(torch.randn([self.batch_size, self.hidden_size]).cuda())\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #TODO - zero \n",
    "        hidden = self.zero_hidden_state()                                            # BxH\n",
    "        cell_state = self.zero_hidden_state()                                        # BxH\n",
    "        encoder_states = []\n",
    "        for j in range(len(inp[0])):                                          # inp -> BxJxI\n",
    "            encoder_input = inp[:, j:j+1]                                            # BxI\n",
    "            hidden, cell_state = self.encode(encoder_input, (hidden, cell_state)) \n",
    "            encoder_states.append(cell_state)\n",
    "            \n",
    "        decoder_state = encoder_states[-1]                       # BxH\n",
    "        pointers = []\n",
    "        pointer_distributions = []\n",
    "        \n",
    "        start_token = 0\n",
    "        decoder_input = Variable(torch.Tensor([start_token] * self.batch_size)        # BxI\n",
    "                                 .view(self.batch_size, self.input_dim).cuda())\n",
    "\n",
    "        for i in range(self.num_of_indices):\n",
    "            hidden = self.zero_hidden_state()                                         # BxH\n",
    "            cell_state = self.zero_hidden_state()                                     # BxH\n",
    "            hidden, cell_state = self.decode(decoder_input, (hidden, cell_state))     # BxH\n",
    "            \n",
    "            decoder_blend = self.blend_decoder(cell_state)                            # BxD\n",
    "            encoder_blends = []\n",
    "            index_predists = []\n",
    "            for i in range(len(inp[0])):\n",
    "                encoder_blend = self.blend_encoder(encoder_states[i])                  # BxD\n",
    "                raw_blend = encoder_blend + decoder_blend                              # BxD\n",
    "                scaled_blend = self.scale_blend(raw_blend).squeeze(1)                  # BxI\n",
    "                \n",
    "                index_predist = scaled_blend\n",
    "                \n",
    "                encoder_blends.append(encoder_blend)\n",
    "                index_predists.append(index_predist)\n",
    "                \n",
    "            index_predistribution = torch.stack(index_predists).t()                    # BxJ\n",
    "            index_distribution = F.log_softmax(index_predistribution)\n",
    "            pointer_distributions.append(index_distribution)                          \n",
    "            index = index_distribution.data.max(1)[1].squeeze(1)                       # B\n",
    "\n",
    "            emb = embedding_lookup(inp.t(), Variable(index))                           # BxB\n",
    "            pointer_raw = torch.diag(emb)                                              # B\n",
    "            pointer = pointer_raw\n",
    "            pointers.append(pointer)\n",
    "            decoder_input = pointer.unsqueeze(1)                                       # Bx1\n",
    "\n",
    "            #print('pointer: {}'.format(pointers))\n",
    "        index_distributions = torch.stack(pointer_distributions)                    \n",
    "        return index_distributions                                                     # NxBxJ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "def train(epochs, model,  train_batches, print_every = 100):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (data, target) in enumerate(train_batches):\n",
    "\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            index_distributions = model(data)\n",
    "            diff = index_distributions -  target\n",
    "            #print(diff[:,:3])\n",
    "            loss = torch.sqrt(\n",
    "                        torch.mean(\n",
    "                            torch.pow(diff, 2)\n",
    "                        )\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            #print([i for i in model.parameters()])\n",
    "            print('epoch: {} -- loss: {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_lookup(embeddings, indices):\n",
    "    result =  embeddings.index_select(0, indices.view(-1))\n",
    "    return result.view(*(indices.size() + embeddings.size()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segment_length_min = 5\n",
    "train_segment_length_max = 7\n",
    "\n",
    "seqs = []\n",
    "start_indices = []\n",
    "end_indices = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    seq, start, end = generate_nested_sequence(max_length, \n",
    "                                                train_segment_length_min, \n",
    "                                                train_segment_length_max)\n",
    "    \n",
    "    start_, end_ = create_one_hot(max_length, start),  create_one_hot(max_length, end)\n",
    "    seqs.append(seq), start_indices.append(start_), end_indices.append(end_)\n",
    "\n",
    "#print(len(seqs))\n",
    "#pprint([len(seq) for seq in seqs])\n",
    "    \n",
    "seqs          = torch.Tensor(seqs)\n",
    "start_indices = torch.Tensor(start_indices)\n",
    "end_indices   = torch.Tensor(end_indices)\n",
    "indices = torch.stack([start_indices, end_indices])\n",
    "train_batches = [(seqs, indices),]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model (\n",
       "  (encode): LSTMCell(1, 6)\n",
       "  (decode): LSTMCell(1, 6)\n",
       "  (blend_decoder): Linear (6 -> 6)\n",
       "  (blend_encoder): Linear (6 -> 6)\n",
       "  (scale_blend): Linear (6 -> 1)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input, sample_output = train_batches[0]\n",
    "model = Model(1, 6, 2, 6, batch_size)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 -- loss: 4.112970352172852\n",
      "epoch: 100 -- loss: 4.112970352172852\n",
      "epoch: 200 -- loss: 4.112970352172852\n",
      "epoch: 300 -- loss: 4.112970352172852\n",
      "epoch: 400 -- loss: 4.112970352172852\n",
      "epoch: 500 -- loss: 4.112969875335693\n",
      "epoch: 600 -- loss: 4.112970352172852\n",
      "epoch: 700 -- loss: 4.112969875335693\n",
      "epoch: 800 -- loss: 4.112970352172852\n",
      "epoch: 900 -- loss: 4.112969875335693\n",
      "epoch: 1000 -- loss: 4.112969875335693\n",
      "epoch: 1100 -- loss: 4.112969875335693\n",
      "epoch: 1200 -- loss: 4.112970352172852\n",
      "epoch: 1300 -- loss: 4.112970352172852\n",
      "epoch: 1400 -- loss: 4.112970352172852\n",
      "epoch: 1500 -- loss: 4.112969875335693\n",
      "epoch: 1600 -- loss: 4.112970352172852\n",
      "epoch: 1700 -- loss: 4.112969875335693\n",
      "epoch: 1800 -- loss: 4.112970352172852\n",
      "epoch: 1900 -- loss: 4.112969875335693\n",
      "epoch: 2000 -- loss: 4.112969875335693\n",
      "epoch: 2100 -- loss: 4.112969875335693\n",
      "epoch: 2200 -- loss: 4.112969875335693\n",
      "epoch: 2300 -- loss: 4.112969875335693\n",
      "epoch: 2400 -- loss: 4.112969875335693\n",
      "epoch: 2500 -- loss: 4.112969875335693\n",
      "epoch: 2600 -- loss: 4.112970352172852\n",
      "epoch: 2700 -- loss: 4.112970352172852\n",
      "epoch: 2800 -- loss: 4.112969875335693\n",
      "epoch: 2900 -- loss: 4.112969875335693\n",
      "epoch: 3000 -- loss: 4.112969875335693\n",
      "epoch: 3100 -- loss: 4.112969875335693\n",
      "epoch: 3200 -- loss: 4.112970352172852\n",
      "epoch: 3300 -- loss: 4.112969875335693\n",
      "epoch: 3400 -- loss: 4.112969875335693\n",
      "epoch: 3500 -- loss: 4.112969875335693\n",
      "epoch: 3600 -- loss: 4.112969398498535\n",
      "epoch: 3700 -- loss: 4.112969875335693\n",
      "epoch: 3800 -- loss: 4.112970352172852\n",
      "epoch: 3900 -- loss: 4.112970352172852\n"
     ]
    }
   ],
   "source": [
    "train(4000, model, train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_segment_length_min = 2\n",
    "test_segment_length_max = 4\n",
    "\n",
    "seqs = []\n",
    "start_indices = []\n",
    "end_indices = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    seq, start, end = generate_nested_sequence(max_length, \n",
    "                                                test_segment_length_min, \n",
    "                                                test_segment_length_max)\n",
    "    \n",
    "    start_, end_ = create_one_hot(max_length, start),  create_one_hot(max_length, end)\n",
    "    seqs.append(seq), start_indices.append(start_), end_indices.append(end_)\n",
    "\n",
    "#print(len(seqs))\n",
    "#pprint([len(seq) for seq in seqs])\n",
    "    \n",
    "seqs          = torch.Tensor(seqs)\n",
    "start_indices = torch.Tensor(start_indices)\n",
    "end_indices   = torch.Tensor(end_indices)\n",
    "indices = torch.stack([start_indices, end_indices])\n",
    "test_batches = [(seqs, indices),]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.17192234098911285\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     2     2     2     2     8     7    10     9     8     3     1     4\n",
      "    4     2     1     4     3     6     6    10    10     6     8     8     1\n",
      "    5     2     5     1     2    10     6     7     9     8     8     4     1\n",
      "    5     4     2     3     3     6     8    10     7    10     7     3     2\n",
      "    2     4     3     1     4     9     7     8     6     6     3     3     3\n",
      "    2     3     4     3     2     5     5     7     7     7     6     8     5\n",
      "    5     1     5     1     1     4     2     9     8    10     6    10     5\n",
      "    4     3     3     2     1     9     9    10    10    10     7     4     4\n",
      "    4     2     1     4     1     5     7     7     7     6    10     4     5\n",
      "    1     1     5     3     4     4     2    10     9     6     7     8     6\n",
      "\n",
      "Columns 13 to 25 \n",
      "    3     4     4     4     0     0     0     0     0     0     0     0     0\n",
      "    2     4     5     2     0     0     0     0     0     0     0     0     0\n",
      "    5     2     2     2     4     0     0     0     0     0     0     0     0\n",
      "    4     2     3     4     4     0     0     0     0     0     0     0     0\n",
      "    2     5     5     0     0     0     0     0     0     0     0     0     0\n",
      "    3     2     5     4     2     4     0     0     0     0     0     0     0\n",
      "    2     3     5     3     4     0     0     0     0     0     0     0     0\n",
      "    2     5     3     1     0     0     0     0     0     0     0     0     0\n",
      "    4     2     2     4     0     0     0     0     0     0     0     0     0\n",
      "    7     1     4     2     2     1     4     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 38 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 39 to 51 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 52 to 59 \n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.cuda.FloatTensor of size 10x60 (GPU 0)]\n",
      "\n",
      "\n",
      "    5\n",
      "    5\n",
      "    5\n",
      "    5\n",
      "    5\n",
      "    7\n",
      "    7\n",
      "    5\n",
      "    6\n",
      "    7\n",
      "[torch.cuda.LongTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "\n",
      "    9\n",
      "   11\n",
      "   10\n",
      "   10\n",
      "    9\n",
      "   11\n",
      "   11\n",
      "   10\n",
      "   10\n",
      "   13\n",
      "[torch.cuda.LongTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "\n",
      "   16\n",
      "   11\n",
      "   11\n",
      "   17\n",
      "   15\n",
      "   18\n",
      "   17\n",
      "   15\n",
      "   16\n",
      "   13\n",
      "[torch.cuda.LongTensor of size 10x1 (GPU 0)]\n",
      "\n",
      "\n",
      "   16\n",
      "   11\n",
      "   11\n",
      "   17\n",
      "   15\n",
      "   18\n",
      "   17\n",
      "   15\n",
      "   16\n",
      "   13\n",
      "[torch.cuda.LongTensor of size 10x1 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-007639dcc6c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mincorrect_pointers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_distributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_distributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "for data, target in train_batches:\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    index_distributions = model(data)\n",
    "    loss = torch.sqrt(\n",
    "                torch.mean(\n",
    "                    torch.pow(index_distributions - target, 2)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    print('loss: {}'.format(loss.data[0]))\n",
    "\n",
    "print(data[:10])\n",
    "print(target[0].data.max(1)[1][:10])\n",
    "print(target[1].data.max(1)[1][:10])\n",
    "print(index_distributions[0].data.max(1)[1][:10])\n",
    "print(index_distributions[1].data.max(1)[1][:10])\n",
    "incorrect_pointers = 0\n",
    "\n",
    "results = index_distributions.data.cpu().numpy()\n",
    "print(index_distributions)\n",
    "for batch_index in range(batch_size):\n",
    "    if batch_index >= 59:\n",
    "        break\n",
    "    print(results[0][batch_index][1])\n",
    "    first_diff = start_[batch_index] - results[1][batch_index][0]\n",
    "    first_diff_max = np.max(np.abs(first_diff))\n",
    "    print(first_diff, first_diff_max)\n",
    "    first_ptr = np.argmax(results[1][batch_index][0])\n",
    "    if first_diff_max >= .5:  # bit stricter than argmax but let's hold ourselves to high standards, people\n",
    "        incorrect_pointers += 1\n",
    "    second_diff = end_[batch_index] - results[1][batch_index][1]\n",
    "    second_diff_max = np.max(np.abs(second_diff))\n",
    "    second_ptr = np.argmax(results[1][batch_index][1])\n",
    "    if second_diff_max >= .5:\n",
    "        incorrect_pointers += 1\n",
    "\n",
    "    print_pointer(seqs[batch_index], first_ptr, second_ptr)\n",
    "    #print(\"\")\n",
    "\n",
    "test_pct = np.round(100.0 * ((2 * batch_size) - incorrect_pointers) / (2 * batch_size), 5)\n",
    "print(\"\")\n",
    "print(\" %s / %s (correct/total); test pct %s\" % ((2*batch_size) - incorrect_pointers,\n",
    "                                                 2 * batch_size,\n",
    "test_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
